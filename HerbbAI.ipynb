{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f2bcf55bd0e4522846c04b654d5a93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e22ce1e278c4ed49b70c8aaf447583e",
              "IPY_MODEL_e37c66831356455d9d4a2b7b45f3bbe6",
              "IPY_MODEL_528617cc9cb44a4fac93152ebc6372d7"
            ],
            "layout": "IPY_MODEL_55d96465a3b54587bfe4324eb3a7435b"
          }
        },
        "2e22ce1e278c4ed49b70c8aaf447583e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb214ca3d6c54ee487bd86a2728aa05b",
            "placeholder": "​",
            "style": "IPY_MODEL_6eea725c83504a3c94c1dd82f2834d2e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e37c66831356455d9d4a2b7b45f3bbe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87f57303f4e140bfab96272c16e58e09",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9af6c24523e41c0a69bd624badc874c",
            "value": 2
          }
        },
        "528617cc9cb44a4fac93152ebc6372d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d362e1e32b1b4c8a8c15f1091649d589",
            "placeholder": "​",
            "style": "IPY_MODEL_2142d0d5c1af4e1d84be1e6740e6f3e8",
            "value": " 2/2 [01:01&lt;00:00, 27.98s/it]"
          }
        },
        "55d96465a3b54587bfe4324eb3a7435b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb214ca3d6c54ee487bd86a2728aa05b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eea725c83504a3c94c1dd82f2834d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87f57303f4e140bfab96272c16e58e09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9af6c24523e41c0a69bd624badc874c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d362e1e32b1b4c8a8c15f1091649d589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2142d0d5c1af4e1d84be1e6740e6f3e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD1gbyZc74bc",
        "outputId": "14dd9437-4b97-4ad5-fd02-4f382953d692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m840.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install accelerate transformers einops langchain xformers bitsandbytes sentence_transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio faiss-gpu  PyMuPDF gdown pypdf -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgFNXVBY8AHl",
        "outputId": "25bcd700-52fd-4a29-8f2d-d76074a1211d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xff8K3n_8AOs",
        "outputId": "f6aa22cb-e2a7-4f88-b144-e0f8b774639a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdeYDG189zCA",
        "outputId": "aa92e3e1-b763-4b1d-88f1-a123895677f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.9/2.4 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer,AutoConfig,AutoModelForCausalLM, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "RAN2WN5V8AVI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Hugging Face Pipeline\n",
        "model_id = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model_config = AutoConfig.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "0f2bcf55bd0e4522846c04b654d5a93c",
            "2e22ce1e278c4ed49b70c8aaf447583e",
            "e37c66831356455d9d4a2b7b45f3bbe6",
            "528617cc9cb44a4fac93152ebc6372d7",
            "55d96465a3b54587bfe4324eb3a7435b",
            "bb214ca3d6c54ee487bd86a2728aa05b",
            "6eea725c83504a3c94c1dd82f2834d2e",
            "87f57303f4e140bfab96272c16e58e09",
            "e9af6c24523e41c0a69bd624badc874c",
            "d362e1e32b1b4c8a8c15f1091649d589",
            "2142d0d5c1af4e1d84be1e6740e6f3e8"
          ]
        },
        "id": "dKQm13KP8Atj",
        "outputId": "1071245e-5368-4fda-d49e-4dc621df3b5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f2bcf55bd0e4522846c04b654d5a93c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "fn9zUKU18bub"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "# Define custom stopping criteria class\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "rphWOQId8b0X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "A1DozU9u8b56"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task='text-generation',\n",
        "    stopping_criteria=stopping_criteria,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "# Initialize LangChain components\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "documents = DirectoryLoader('/content/drive/MyDrive/data', glob=\"./*.pdf\", loader_cls=PyPDFLoader).load()\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "vectorstore = FAISS.from_documents(all_splits, embeddings)\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)\n",
        "chat_history = []\n",
        "\n",
        "# Define the Gradio interface function\n",
        "def answer_question(query):\n",
        "    result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "    chat_history.append((query, result[\"answer\"]))\n",
        "    return result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR-bHEfx8cBo",
        "outputId": "a3d03647-2883-4e59-f7bd-aaa1763f2ec5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7e6331ef052b>:13: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=generate_text)\n",
            "<ipython-input-6-7e6331ef052b>:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_question(\"Remedies for PCOD\"))"
      ],
      "metadata": {
        "id": "FX_UYolbLHYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from datetime import datetime\n",
        "import gradio as gr\n",
        "\n",
        "# Define answer generation function (assuming you already have `chain` defined)\n",
        "def answer_question(query):\n",
        "    # Establish a persistent connection to a file-based SQLite database\n",
        "    conn = sqlite3.connect(\"chat_history.db\")  # Replace \":memory:\" with a file-based DB\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Ensure the table exists\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS chat_history (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            question TEXT,\n",
        "            answer TEXT,\n",
        "            timestamp TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    # Answer the question\n",
        "    result = chain({\"question\": query, \"chat_history\": []})\n",
        "    answer = result[\"answer\"]\n",
        "\n",
        "    # Insert question and answer into SQLite\n",
        "    cursor.execute(\"INSERT INTO chat_history (question, answer, timestamp) VALUES (?, ?, ?)\",\n",
        "                   (query, answer, str(datetime.now())))\n",
        "    conn.commit()\n",
        "\n",
        "    # Close the connection\n",
        "    conn.close()\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Function to get chat history from SQLite\n",
        "def format_chat_history():\n",
        "    # Establish a persistent connection to fetch data and ensure the table exists\n",
        "    conn = sqlite3.connect(\"chat_history.db\")\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Ensure the table exists (will be created if it doesn't)\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS chat_history (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            question TEXT,\n",
        "            answer TEXT,\n",
        "            timestamp TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"SELECT question, answer, timestamp FROM chat_history\")\n",
        "    rows = cursor.fetchall()\n",
        "    formatted_history = \"\"\n",
        "    for row in rows:\n",
        "        formatted_history += f\"**Question**: {row[0]}\\n\"\n",
        "        formatted_history += f\"**Answer**: {row[1]}\\n\"\n",
        "        formatted_history += f\"**Timestamp**: {row[2]}\\n\\n\"\n",
        "\n",
        "    # Close the connection\n",
        "    conn.close()\n",
        "\n",
        "    return formatted_history\n",
        "\n",
        "# Gradio interface function\n",
        "def chatbot_interface(user_query):\n",
        "    response = answer_question(user_query)\n",
        "    formatted_history = format_chat_history()\n",
        "    return response, gr.update(value=formatted_history)\n",
        "\n",
        "# Gradio setup\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# HerbAI 🌿 - Your Ayurveda Assistant\")\n",
        "\n",
        "    with gr.Row():\n",
        "        user_input = gr.Textbox(placeholder=\"Ask a question about Ayurveda...\", label=\"Your Question\")\n",
        "        output = gr.Textbox(label=\"Answer\")\n",
        "\n",
        "    history_output = gr.Textbox(label=\"Chat History\", value=format_chat_history(), interactive=False)\n",
        "\n",
        "    # Link the submit function\n",
        "    user_input.submit(chatbot_interface, user_input, [output, history_output])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "9NdH7icsMd6-",
        "outputId": "cc5f5ca2-84fe-4831-b1a1-165a3d5d748a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4db2fffc62d02daf49.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4db2fffc62d02daf49.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import gradio as gr\n",
        "\n",
        "# Function to load chat history from JSON\n",
        "def load_chat_history(file_path=\"chat_history.json\"):\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return []\n",
        "\n",
        "# Function to save chat history to JSON\n",
        "def save_chat_history(history, file_path=\"chat_history.json\"):\n",
        "    with open(file_path, \"w\") as f:\n",
        "        json.dump(history, f, indent=4)\n",
        "\n",
        "# Format the chat history for display\n",
        "def format_chat_history(history):\n",
        "    formatted_history = \"\"\n",
        "    for entry in history:\n",
        "        formatted_history += f\"**Question**: {entry['question']}\\n\"\n",
        "        formatted_history += f\"**Answer**: {entry['answer']}\\n\"\n",
        "        formatted_history += f\"**Timestamp**: {entry['timestamp']}\\n\\n\"\n",
        "    return formatted_history\n",
        "\n",
        "# Initialize chat history\n",
        "chat_history = load_chat_history()\n",
        "\n",
        "# Define answer generation function\n",
        "def answer_question(query):\n",
        "    # Use your existing function to get the answer\n",
        "    result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "    answer = result[\"answer\"]\n",
        "    chat_history.append({\"question\": query, \"answer\": answer, \"timestamp\": str(datetime.now())})\n",
        "    save_chat_history(chat_history)\n",
        "    return answer\n",
        "\n",
        "# Gradio function to handle user query and display chat history\n",
        "def chatbot_interface(user_query):\n",
        "    response = answer_question(user_query)\n",
        "    formatted_history = format_chat_history(chat_history)  # Format chat history for display\n",
        "    return response, formatted_history\n",
        "\n",
        "# Gradio interface with chat display and chat history\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# HerbAI 🌿 - Your Ayurveda Assistant\")\n",
        "    with gr.Row():\n",
        "        user_input = gr.Textbox(placeholder=\"Ask a question about Ayurveda...\", label=\"Your Question\")\n",
        "        output = gr.Textbox(label=\"Answer\")\n",
        "    history_output = gr.Textbox(label=\"Chat History\", value=format_chat_history(chat_history), interactive=False)\n",
        "\n",
        "    def handle_submit(user_query):\n",
        "        response, formatted_history = chatbot_interface(user_query)\n",
        "        history_output.value = formatted_history  # Update chat history display\n",
        "        return response\n",
        "\n",
        "    user_input.submit(handle_submit, user_input, [output])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "yb8PRHCI6xpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import time\n",
        "#import gradio as gr\n",
        "\n",
        "# Define your answer generation function\n",
        "#def slow_echo(question, history=[]):\n",
        "    #history = history or []\n",
        "    #for i in range(len(question)):\n",
        "        #ans = answer_question(question)\n",
        "        #response = \"Answer: \" + ans\n",
        "        #history.append((question, response))\n",
        "        #yield history  # Yield the history for continuous updates\n",
        "\n",
        "# Customize title, description, and examples\n",
        "#title = \"HerbAI 🌿 - Your Ayurveda Assistant\"\n",
        "#description = \"\"\"\n",
        "#Welcome to HerbAI🌿! I'm here to assist with your Ayurvedic health questions.\n",
        "#Feel free to ask about remedies, health practices, or doshic types.\n",
        "#\"\"\"\n",
        "\n",
        "# examples = [\n",
        "#    \"What should we eat in order to overcome constipation?\",\n",
        "#    \"What are the steps to determine one's prakruti and vikruti according to Ayurveda?\",\n",
        "#    \"What are the characteristics of each doshic type in Ayurveda?\",\n",
        "#    \"Explain breathing techniques in Ayurveda\"\n",
        "#]\n",
        "\n",
        "# Setting up the Chat Interface\n",
        "#demo = gr.ChatInterface(\n",
        "#    fn=slow_echo,\n",
        " #   title=title,\n",
        "#    description=description,\n",
        "#    examples=examples,\n",
        "#    theme=\"default\",\n",
        "#).queue()\n",
        "\n",
        "# Launch the interface\n",
        "#if __name__ == \"__main__\":\n",
        "#    demo.launch(share=True,debug=True)"
      ],
      "metadata": {
        "id": "DKcc5_NC612o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}